{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "## 01 半监督学习\n",
    "## 02 生成模型和判别模型\n",
    "## 03 LR模型\n",
    "## 04 最大熵\n",
    "## 05 信息增益\n",
    "## 06 梯度下降\n",
    "## 07 交叉熵函数求导\n",
    "## 08 softmax函数求导\n",
    "## 09 Sigmod函数与softmax函数的联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01半监督学习\n",
    "参考 https://www.cnblogs.com/kamekin/p/9683162.html\n",
    "\n",
    "### 1-1、什么是半监督学习\n",
    "\n",
    "#### 1.1.1 定义。\n",
    "\n",
    "让学习器不依赖外界交互，自动地利用未标记样本来提升学习性能，就是半监督学习（semi-supervised learning）。\n",
    "\n",
    "半监督学习 假设“相似的样本拥有相似的输出”。\n",
    "\n",
    "#### 1.1.2 分类。\n",
    "\n",
    "纯（pure）半监督学习。假定训练数据中的未标记样本 不是 准备进行预测的数据，\n",
    "\n",
    "直推学习（transductive learning）。假定学习过程中所使用的未标记样本恰是 准备进行预测数据。等于需要使用部分待预测数据，预测这些数据本身。\n",
    "\n",
    "### 1-2、无标记样本的意义\n",
    "\n",
    "有了无标签数据的分布信息后，两个类的分类超平面就变得比较明确了。**因此，使用无标签数据可以提高分类边界的准确性，提高模型的稳健性。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3、伪标签（Pseudo-Labelling）学习\n",
    "\n",
    "#### 1.3.1 定义\n",
    "伪标签学习也叫简单自训练（simple self-training）：用有标签数据训练一个分类器，然后用这个分类器对无标签数据进行分类，\n",
    "\n",
    "这样就会产生伪标签（pseudo label）或软标签（soft label），挑选你认为分类正确的无标签样本（此处应该有一个挑选准则），把选出来的无标签样本用来训练分类器\n",
    "\n",
    "#### 1.3.2 大致步骤\n",
    "\n",
    "1)用有标签数据训练模型；\n",
    "\n",
    "2)用训练的模型为无标签的数据预测标签，即获得无标签数据的伪标签；\n",
    "\n",
    "3)使用(2)获得的伪标签和标签数据集重新训练模型；\n",
    "\n",
    "最终的模型是(3)训练得到，用于对测试数据的最终预测。\n",
    "\n",
    "在实际使用过程中，会在(3)步中增加一个参数：采样比例（sample_rate），表示无标签数据中本用作伪标签样本的比率。\n",
    "\n",
    "\n",
    "### 1.4 半监督学习方法 \n",
    "#### 1.4.1 半监督SVM（Semi-Supervised Support Vector Machine,简称S3VM）\n",
    "\n",
    "#### 传统SVM和S3VM的区别\n",
    "有传统SVM试图找到一个划分超平面，使得两侧支持向量之间的间隔最大，即“最大划分间隔”思想。</p>\n",
    "半监督学习，S3VM则考虑超平面需穿过数据低密度的区域。\n",
    "\n",
    "#### TSVM。\n",
    "定义。TSVM是半监督支持向量机中的最著名代表。\n",
    "\n",
    "主要思想。尝试将每个未标记样本分别作为正例或反例，在所有结果中，寻找一个在所有样本上间隔最大的划分超平面。\n",
    "\n",
    "求解步骤。采用局部搜索的策略来进行迭代求解。\n",
    "\n",
    "1) 首先使用有标记样本集训练出一个初始SVM。</p>\n",
    "2) 接着使用该学习器对未标记样本进行标记，这样所有样本都有了标记。 </p>\n",
    "\n",
    "3) 基于这些有标记的样本重新训练SVM，之后再寻找易出错样本不断调整。</p>\n",
    "#### 1.4.2 半监督深度学习 \n",
    "详见参考文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 生成模型和判别模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-1 生成模型。\n",
    "功能。可以生成数据，比如 图片，音乐，文本。  \n",
    "数学理解。最大化 P(x)，P(xy)。最大化目标函数  \n",
    "问题：怎么通过生成模型分辨猫和狗？  \n",
    "回答：通过知道猫和狗的特点，判断是猫的概率多少，狗的概率是多少。  \n",
    "## 02-2 判别模型。\n",
    "功能。通过记录样本的区别。  \n",
    "数学理解。最大化P(y|x)，最大化条件概率。  \n",
    "问题：怎么通过判别模型分辨猫和狗？  \n",
    "回答：通过记录猫和狗的区别。  \n",
    "\n",
    "案例。判断一直羊是山羊还是绵羊。  \n",
    "判别模型。历史是山羊还是绵羊的样本中，提取特征学习出模型。然后将新羊的特征，输入这个模型，得到是山羊还是绵羊。  \n",
    "生成模型。提取山羊的特征，学习出山羊模型；提取绵羊特征，学习出绵羊模型。将新羊的特征，放到山羊模型和绵羊模型，看哪个模型概率大，就是哪种羊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 LR 模型\n",
    "### 问题1. 能不能用线性回归来表示  P(Y|X) = wx + b  \n",
    "不能。wx+b的值域是$(-\\infty,+\\infty)$,P(Y|X)的值域是[0,1]。P(Y|X)的和要等于1.\n",
    "\n",
    "### 问题2. 逻辑回归是线性分类器？怎么判断一个模型是否是线性分类器？什么是决策边界？怎么求解决策边界？\n",
    "是线性分类器。主要看决策边界。P(Y=1|X) = P(Y=0|X) --> WX + B == 0,这是一个线性解。  \n",
    "\n",
    "### 问题3. 逻辑回归的目标函数是什么样的？我们一般会做什么样的化简？目标函数和损失函数的区别是什么？\n",
    "#### 目标函数。\n",
    "针对数据集，我们有 $p(y|x,w) = p(y=1|x,w)^y [1 - p(y=1|x,w)]^{1-y}，$\n",
    "\n",
    "$需要最大化目标函数 \\hat{W}_{MLE},\\hat{b}_{MLE} = argmax_{W}\\prod_{i=1}^{n}p(y_i|x_i:w,b)，即让目标函数最大的w,b$\n",
    "\n",
    "#### 化简操作：    \n",
    "实质是通过log把 Max连乘 --> log Max连加 --> - log Min\n",
    "$ argmax_{w,b}\\prod_{i=1}^{n}p(y_i|x_i:w,b) $  \n",
    "\n",
    "$ = argmax_{w,b}log[ \\prod_{i=1}^{n} p(y_i|x_i:w,b) ] $  \n",
    "\n",
    "$ = argmax_{w,b}\\sum_{i=1}^{n} log[p(y_i|x_i:w,b)] $  \n",
    "\n",
    "$ = argmin_{w,b} -\\sum_{i=1}^{n} log[p(y_i|x_i:w,b)] $\n",
    "\n",
    "#### 目标函数和损失函数的区别？\n",
    "\n",
    "Loss Function 是定义在单个样本上的，算的是一个样本的误差。  \n",
    "Cost Function 是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。  \n",
    "Object Function（目标函数 ）定义为：Cost Function + 正则化项。目标函数是最终要达到的目标。\n",
    "\n",
    "\n",
    "\n",
    "### 问题4. 逻辑回归的目标函数是凸函数吗？什么是凸函数？凸函数有什么性质？为什么模型经常使用凸函数作为目标函数？\n",
    "是凸函数。凸函数，两点的割线始终位于两点的函数曲线的上方。因为凸函数的极值点就是全局最优解。\n",
    "\n",
    "### 问题5. 什么是梯度下降算法？以f(w) = 4w^2 + 5w + 1 为例，初始化w0=0，求解w。请推导  σf(w,b)/σw 和  σf(w,b)/σb\n",
    "\n",
    "前提公式。1)log乘法变加法。log(x*y) = log(x)+log(y)。2)求导log变除法。(log(x))^ = 1/x  3) 求导。(xy)^ = x^y + xy^\n",
    "\n",
    "\n",
    "### 问题6. 什么是随机梯度下降法？它和梯度下降算法的时间复杂度分别是多少？为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
