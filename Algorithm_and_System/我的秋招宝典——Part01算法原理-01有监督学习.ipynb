{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "### 1.1 LR\n",
    "### 1.2 SVM\n",
    "### 1.3 决策树\n",
    "### 1.4 集成模型 xgb rf lgb\n",
    "### 1.5 KNN\n",
    "### 1.6 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================================\n",
    "===============================================================\n",
    "## 1.1 逻辑回归问题集\n",
    "参考 一文详尽系列之逻辑回归\n",
    "https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247487494&idx=1&sn=01cc822880d5ded2a3685bf88d9fe25a&chksm=e870c14bdf07485ded9589fc0f0533e4bb0cc66cb0ed849a51a3c78df7439be6acfb6397216d&mpshare=1&scene=1&srcid=&sharer_sharetime=1574834010999&sharer_shareid=ce71cb8916e6563c3a833837c5e9bc18&key=a3e7a9e6351a3fda00b7cb7ae119d4800a41a074da65892c0998c2d85ea08e59142de6df2f1a3a2db35487d35906e89e6c15afa2800850ebd9c80ca3a9cc83a6617a41d982aaa00cdaf3f04961bf42b6&ascene=1&uin=MTY0Mjc4NDkwMA%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&pass_ticket=1Say5lfE1%2FWnnYnvV%2Fo%2BEG25lxuQly5qT76ESRzqNbmW4IOQW2wESAlI%2BIMWln%2FB\n",
    "\n",
    "一文读懂线性回归、岭回归和Lasso回归，算法面试必备！\n",
    "https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247486680&idx=1&sn=d6c470224a1703f85f7ff098f8b7c6d9&chksm=e870dd95df075483597c98dfda79469c435e8af06a0ead6a637c0c98c1b8f9f3a990689426d9&mpshare=1&scene=1&srcid=&sharer_sharetime=1574834139061&sharer_shareid=ce71cb8916e6563c3a833837c5e9bc18&key=c6adf044e136fdb9224998bdb82e6a4501c27c75a7eac9e561f841962100277ab75dc92d45013dee4bbc4d890ef9316f729b9d3b8e3b4fbaaadeb7f7ef5ab345f220ffa45c2cb36f027e494f069e75bf&ascene=1&uin=MTY0Mjc4NDkwMA%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&pass_ticket=1Say5lfE1%2FWnnYnvV%2Fo%2BEG25lxuQly5qT76ESRzqNbmW4IOQW2wESAlI%2BIMWln%2FB\n",
    "\n",
    "#### 1. 逻辑回归的损失函数为什么用KL散度？能写一下逻辑回归的损失函数吗，为什么不用MSE(L2 loss)作为损失函数?\n",
    "\n",
    "参考 《交叉熵、相对熵（KL散度）、JS散度和Wasserstein距离（推土机距离）》  \n",
    "https://zhuanlan.zhihu.com/p/74075915\n",
    "熵的故事-爱因斯坦打拳击\n",
    "1. 信息量。爱因斯坦 拳头信息 = -log(概率)\n",
    "2. 熵。爱因斯坦出拳的全部信息。爱因斯坦每次出拳 = 概率*拳头信息，全部出拳加起来叫做熵。拳头信息 -log(概率)\n",
    "3. 相对熵。爱因斯坦终极拳击，叫做相对熵，也叫KL散度。对比 爱因斯坦打拳 和 教学打拳(真实分布)的差距。\n",
    "概率*log(概率/真实概率)。\n",
    "4. 交叉熵。相对熵 可以拆解成 真实分布熵 + 交叉熵。交叉熵 -概率*log(真实概率)\n",
    "\n",
    "\n",
    "##### KL散度\n",
    "1）定义。KL散度也称KL距离，是利用熵的来表示分布之间的差距，也叫做相对熵(relative entropy)。   \n",
    "2）逻辑回归中的损失函数。逻辑回归损失函数使用的不是KL散度，而是交叉熵。   \n",
    "3）交叉熵与相对熵的关系。\n",
    "交叉熵是相对熵做变换后分成的两部分 相对熵 = 真实分布P的熵 + 交叉熵。  \n",
    "真实分布P的熵是确定的，因此 在衡量 机器学习的LOSS时，两者等价。  \n",
    "\n",
    "\n",
    "最大似然方法的推导可以参考：  \n",
    "https://blog.csdn.net/iterate7/article/details/78992027\n",
    "\n",
    "###### 损失函数如下所示：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{Cost(h_\\theta(x^{(i)},y^{(i)}))} \\\\\n",
    "&Cost(h_\\theta(x,y)) = - \\log(h_\\theta(x))  &if \\quad y = 1 \\\\\n",
    "&Cost(h_\\theta(x,y)) = - \\log(1 - h_\\theta(x))  &if \\quad y = 0 \\\\\n",
    "&J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}{[y_i\\log(h_\\theta(x^{(i)})) + (1-y_i)\\log(1 - h_\\theta(x^{(i)}))]} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "##### 不用MSE做损失函数的原因：\n",
    "\n",
    "1. 因为我们想要让 每一个 样本的预测都要得到最大的概率，即将所有的样本预测后的概率进行相乘都最大，也就是极大似然函数.  \n",
    "2. 对极大似然函数取对数以后相当于对数损失函数，由 梯度更新 的公式可以看出，对数损失函数的训练求解参数的速度是比较快的，而且更新速度只和x，y有关，比较的稳定。  \n",
    "3. 为什么不用平方损失函数呢，如果使用平方损失函数，梯度更新的速度会和 sigmod 函数的梯度相关，sigmod 函数在定义域内的梯度都不大于0.25，导致训练速度会非常慢。而且平方损失会导致损失函数是 theta 的非凸函数，不利于求解，因为非凸函数存在很多局部最优解  \n",
    "\n",
    "\n",
    "#### 2.逻辑回归和最大似然有什么关系？\n",
    "\n",
    "逻辑回归 输出 $y=\\frac{1}{1+e^{-\\theta^Tx}}$ 可看作后验概率估计 $p(y=1 |x)$, 则 $p(y=0|x) = 1 - y$。通过极大似然法可以求解逻辑回归模型中的参数。\n",
    "\n",
    "#### 3.逻辑回归用梯度下降优化，学习率对结果有什么影响？\n",
    "a.学习率过低则模型训练速度会慢\n",
    "b.学习率过高则模型训练会在全局最优点附近震荡，甚至不收敛\n",
    "\n",
    "#### 4.逻辑回归中样本不均衡我们怎么处理？\n",
    "a. 调整分类阈值，不统一使用 0.5，根据样本中类别的比值进行调整。\n",
    "b.多类样本负采样。进一步也可将多类样本负采样构建多个训练集，最后聚合多个模型的结果。\n",
    "c.少类样本过采样。过采样的方法大致有三种：\n",
    "\n",
    "​    c1: 随机复制\n",
    "\n",
    "​    c2: 基于聚类的过采样\n",
    "\n",
    "​    c3: SMOTE\n",
    "\n",
    "d. 改变性能指标，推荐采用 ROC AUC、F1 Score4.\n",
    "\n",
    "e.模型训练增加正负样本惩罚权重,少类样本权重加大，增大损失项。\n",
    "\n",
    "#### 5.逻辑回归的训练集数据如果做过采样，怎么对预估的概率p做变换才能还原真实概率?\n",
    "\n",
    "$$\n",
    "q = \\frac{p}{p+(1 - p)\\lambda}\\\\\n",
    "\\lambda = \\frac{1-\\tau}{\\tau} . \\frac{\\overline y}{1- \\overline y}\\\\[0.1in]\n",
    "\\tau : 未 进 行 采 样 正 样 本 比 例  \\\\[0.1in]\n",
    "\\overline y ：采 样 后 正 样 本 比 例 \\\\[0.1in]\n",
    "q: 还 原 后 真 实 概 率 \\\\[0.1in]\n",
    "p: 采 样 后 预 估 概 率\n",
    "$$\n",
    "\n",
    "#### 6.L1和L2正则化有什么区别\n",
    "\n",
    "以逻辑回归为例，首先正则化都是为了限制模型参数的搜索空间，缓解过拟合\n",
    "\n",
    "L1的形式是一范数(绝对值)，L2的形式是二范数(平方)\n",
    "\n",
    "L1正则化有截断效应，容易产出稀疏解，有特征选择的功能；L2正则化有缩放效应，倾向于产出很多幅度不大的参数w；\n",
    "\n",
    "从贝叶斯的角度看，L1正则化相当于对数据加了一个拉普拉斯先验，L2正则化相当于高斯先验\n",
    "\n",
    "关于L1正则化为什么容易产出稀疏解，请参考[机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)\n",
    "\n",
    "加入L1 损失函数简化理解为： J = |w1| + |w2|. 是一个类似中心点在原点的菱形。求解 J使用梯度下降法，是类似一个等高线的波纹圆，两者相交的第一个点，就是最优点，很容易落在坐标轴上，这时候就容易产生稀疏解。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ===================================================================================================================================\n",
    "  ===================================================================================================================================\n",
    "  \n",
    " ### 1.2 SVM习题集\n",
    "\n",
    "#### 1.SVM的原理是什么？\n",
    "\n",
    "SVM是一种二分类模型，学习的目标是在特征空间中找到一个分离超平面，且此超平面是间隔最大化的最优分离超平面，最终可转化为一个凸二次规划问题求解。\n",
    "\n",
    "#### 2.SVM为什么采用间隔最大化？\n",
    "\n",
    "间隔最大化使超平面所产生的结果是最鲁棒的，对未见示例的泛化能力最强。\n",
    "\n",
    "#### 3.为什么要将求解SVM的原始问题转换为其对偶问题？\n",
    "\n",
    "对偶问题往往更容易求解。\n",
    "\n",
    "#### 4.为什么SVM要引入核函数？\n",
    "\n",
    "原始样本空间可能会线性不可分，这样需要将原始空间映射到一个更高维的特征空间，使得样本在这个特征空间线性可分。样本映射到高维空间后的内积求解通常是困难的，引入核函数可简化内积的计算。\n",
    "\n",
    "#### 5.SVM有哪些常见核函数，形式是什么？\n",
    "\n",
    "线性核 $k(x_i,x_j) = x_i^Tx_j$\n",
    "\n",
    "多项式核 $ k(x_i,x_j) = (x_i^Tx_j)^d   $  d>=1为多项式的次数\n",
    "\n",
    "高斯核 $k(x_i,x_j) = exp(- \\frac{||x_i-x_j||^2}{2\\sigma^2}) $ $\\sigma >0 为高斯核的宽带$\n",
    "\n",
    "#### 6.什么样的函数可以作为SVM的核函数？\n",
    "\n",
    "由于一般我们说的核函数都是正定核函数，这里我们直说明正定核函数的充分必要条件。一个函数要想成为正定核函数，必须满足他里面任何点的集合形成的Gram矩阵是半正定的。也就是说,对于任意的$x_i \\in X,i=1,2,3…m,K(x_i,x_j)$对应的Gram矩阵$K=[K(x_i,x_j)]$是半正定矩阵，则$K(x,z)$是正定核函数。\n",
    "\n",
    "#### 7.SVM对缺失数据敏感吗，为什么？\n",
    "\n",
    "敏感。SVM模型涉及到样本点“距离”的度量，缺失数据会非常重要。\n",
    "\n",
    "#### 8.SVM的损失函数是什么形式？可以用梯度下降优化吗？\n",
    "\n",
    "SVM的损失函数是hinge损失。hinge损失函数是凸函数，可以使用梯度下降进行优化。由于hinge损失函数不可导，可采用次梯度下降法进行优化。\n",
    "\n",
    "#### 9.LR和SVM有什么区别，分别在什么场景下使用？\n",
    "\n",
    "a.支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用，虽然作用会相对小一些。\n",
    "\n",
    "b.SVM是结构最小化算法，LR需另外在损失函数上添加正则项。\n",
    "\n",
    "c.LR输出具有自然的概率意义，支持向量机输出不具有概率意义。\n",
    "\n",
    "d.SVM采用核函数解决非线性问题，LR通常不采用核函数的方法。\n",
    "\n",
    "##### ！！场景使用 摘自《知乎》，不知道对不对？？？\n",
    "\n",
    "假设： n = 特征数量，m = 训练样本数量\n",
    "\n",
    "1）如果n相对于m更大，比如 n = 10,000，m = 1,000，则使用 LR 或线性 SVM\n",
    "\n",
    "理由：特征数相对于训练样本数已经够大了，使用线性模型就能取得不错的效果，不需要过于复杂的模型；\n",
    "\n",
    "2）如果n较小，m比较大，比如n = 10，m = 10,000，则使用SVM（高斯核函数）\n",
    "\n",
    "理由：在训练样本数量足够大而特征数较小的情况下，可以通过使用复杂核函数的SVM来获得更好的预测性能，而且因为训练样本数量并没有达到百万级，使用复杂核函数的SVM也不会导致运算过慢；\n",
    "\n",
    "3）如果n较小，m非常大，比如n = 100, m = 500,000，则应该引入／创造更多的特征，然后使用lr或者线性核函数的SVM\n",
    "\n",
    "理由：因为训练样本数量特别大，使用复杂核函数的SVM会导致运算很慢，因此应该考虑通过引入更多特征，然后使用线性核函数的SVM或者lr来构建预测性更好的模型。\n",
    "\n",
    "#### 10.SVM的高斯核为什么会把原始维度映射到无穷多维？\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "K(x,x')  &= exp(-(x-x')^2)\\\\\n",
    "&= exp(-(x)^2)exp(-(x')^2)exp(2xx')\\\\\n",
    "&= exp(-(x)^2)exp(-(x')^2)(\\sum_{i=0}^{\\infty}{\\frac{(2xx')^i}{i!}})\\\\\n",
    "&= \\sum_{i=0}^{\\infty}{\\left(exp(-(x)^2)exp(-(x')^2)\\sqrt{\\frac{2!}{i!}}\\sqrt{\\frac{2!}{i!}}(x)^i(x')^i\\right)}\\\\\n",
    "&= \\Phi(x)^T\\Phi(x')\\\\\n",
    "\\Phi(x) &= exp(-(x)^2).(1,\\sqrt{\\frac{2}{1!}},\\sqrt{\\frac{2^2}{2!}},...)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "展开后的特征是无限维的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================================================\n",
    "==============================================================\n",
    "## 1.3 决策树\n",
    "### 决策树习题集\n",
    "\n",
    "参考 最常见核心的决策树算法—ID3、C4.5、CART（非常详细\n",
    "\n",
    "https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247486796&idx=1&sn=08add8f76894f4f39fa95d78876254fb&chksm=e870dc01df075517152e13d86312fcc25ed874669f3cddd6bded97eb1b4783114a792a90ee7f&mpshare=1&scene=1&srcid=&sharer_sharetime=1574834120076&sharer_shareid=ce71cb8916e6563c3a833837c5e9bc18&key=9df239ea85cb34f0629f6acf027d57b8482806987580dbafd17861410f24799215af928af6374d749df689cd6b544de9706943430a9a7c92407fcc5395931174f4779d6bcbcab9e03427fefd3ce41355&ascene=1&uin=MTY0Mjc4NDkwMA%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&pass_ticket=1Say5lfE1%2FWnnYnvV%2Fo%2BEG25lxuQly5qT76ESRzqNbmW4IOQW2wESAlI%2BIMWln%2FB\n",
    "\n",
    "最常用的决策树算法！Random Forest、Adaboost、GBDT 算法\n",
    "https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247486989&idx=1&sn=65788f9172562ca3073f877c789a8785&chksm=e870df40df075656e0c214d573d58755a5a031d5ba6aa237fe27459a58639de0652c8dabde82&mpshare=1&scene=1&srcid=&sharer_sharetime=1574834127711&sharer_shareid=ce71cb8916e6563c3a833837c5e9bc18&key=615f057d39a8b80a2999fcae97285f5847d9e5871b37628bfe41a3807c8cee10e4e033d8509d4ac260f67a5a301cd9166fe776ac5a0e38a2c8ba2e19cd8f021a64593b90bcd0f7928bae9b3d446e4577&ascene=1&uin=MTY0Mjc4NDkwMA%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&pass_ticket=1Say5lfE1%2FWnnYnvV%2Fo%2BEG25lxuQly5qT76ESRzqNbmW4IOQW2wESAlI%2BIMWln%2FB\n",
    "\n",
    "#### 1.解释一下决策树的建模过程？\n",
    "\n",
    "分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶节点表示一个类。\n",
    "\n",
    "用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点；每一个子节点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶节点。最后将实例分到叶节点的类中。\n",
    "\n",
    "决策树学习算法包含特征选择、决策树的生成和决策树的剪枝过程。\n",
    "\n",
    "\n",
    "\n",
    "#### 2.有几种不同的决策树，区别在哪？\n",
    "\n",
    "ID3，C4.5，CART\n",
    "\n",
    "| 算法 | 支持模型   | 树结构 | 特征选择                                                     | 连续值处理 | 缺失值处理 | 剪枝                        |\n",
    "| ---- | ---------- | ------ | ------------------------------------------------------------ | ---------- | ---------- | --------------------------- |\n",
    "| ID3  | 分类       | 多叉树 | 信息增益                                                     | 不支持     | 不支持     | 不支持                      |\n",
    "| C4.5 | 分类       | 多叉树 | 信息增益比（实际先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益率最高的） | 支持       | 支持       | 支持（正则系数a为人为指定） |\n",
    "| CART | 分类、回归 | 二叉树 | 基尼系数，和方差                                             | 支持       | 支持       | 支持（正则系数a自动求解）   |\n",
    "\n",
    "#### 3.决策树的缺失值和数值型特征分别是怎么处理的？\n",
    "\n",
    "##### 数值型特征：\n",
    "\n",
    "采用二分法对连续属性进行处理。给定样本集D和连续属性a，假定a在D上出现了n个不同的取值，将这些值从小到大排序，记为$\\{a^1,a^2,…,a^n\\}$。对连续属性a，我们可考察包含n-1个元素的候选划分点集合\n",
    "$$\n",
    "T_a= \\{ \\frac{a^i+a^{i+1}}{2}| 1 \\leq i \\leq n-1\\}\n",
    "$$\n",
    "基于划分点 t 可将 D 分为$D_t^{-} , D_t^{+}$两个集合。然后像离散属性值一样来考察这些划分点，选取最优的划分点进行样本的划分。\n",
    "\n",
    "注意：与离散属性不同，若当前节点划分属性为连续属性，该属性可作为其后代节点的划分属性。\n",
    "\n",
    "\n",
    "\n",
    "##### 缺失值：\n",
    "\n",
    "缺失值影响决策树有两个方面：\n",
    "\n",
    "###### a.如何在属性值缺失的情况下进行划分属性的选择？\n",
    "\n",
    "采用以下公式进行属性信息增益的计算：\n",
    "$$\n",
    "Gain(D,a) = \\rho \\times Gain(\\tilde D,a)\\\\\n",
    "= \\rho \\times (Ent(\\tilde D)- \\sum_{v=1}^{V}{\\tilde r_v(Ent(\\tilde D)}))\\\\\n",
    "\\rho:无缺失样本值所占的比例 \\\\\n",
    "\\tilde r_v:无缺失样本中在属性a上取值a^v 的样本所占的比例\n",
    "$$\n",
    "###### b.给定划分属性，若样本在该属性上的值缺失，如何划分样本？\n",
    "\n",
    "这种情况下将样本x同时划入所有子节点，且样本权值在与属性值$a^v$对应的子节点中调整为$\\tilde r_v . w_x$。（参见西瓜书）\n",
    "\n",
    "\n",
    "\n",
    "#### 4.决策树是如何完成回归的？\n",
    "\n",
    "决策树回归过程与分类类似，主要区别在决策树利用平方误差作为属性划分的依据，找到最优切分属性和切分点。最后的输出为叶节点的所有样本的平均值。\n",
    "\n",
    "回归树是一种自顶向下的贪婪是递归二分方案\n",
    "\n",
    "#### 5.决策树怎么控制过拟合?\n",
    "\n",
    "决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即过拟合现象。过拟合的原因在于决策树过于复杂，通过剪枝，从已生成的树上裁剪掉一些子树或叶结点，并将其根节点或父节点作为新的叶结点，从而简化树模型，降低过拟合。\n",
    "\n",
    "常用的剪枝方法有预剪枝和后剪枝。\n",
    "\n",
    "我们还可以通过控制树深，对样本或特征进行采样，控制分裂叶子节点最少样本数，最小gain提升等来控制过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================================================================================\n",
    "==============================================================\n",
    "\n",
    "### 1.4 XGBoost习题集\n",
    "参考 终于有人把XGBoost 和 LightGBM 讲明白了，项目中最主流的集成算法\n",
    "https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247487013&idx=1&sn=8eac00bf8e78768b5da58789eec91435&chksm=e870df68df07567ecd3853665969a7bb43d25bf58a38c630d1004503eaaa992b3b4cc8ccdc61&mpshare=1&scene=1&srcid=&sharer_sharetime=1574834084260&sharer_shareid=ce71cb8916e6563c3a833837c5e9bc18&key=615f057d39a8b80a498e6f06825a0b8c4a6c246aa4bedaed7ec1bf2c1d30a17ee4e461005eb12665efd86d2505b969039ab927a1691478a32584a1610947f7486bd3713c1e01b7570a1293618fd34407&ascene=1&uin=MTY0Mjc4NDkwMA%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&pass_ticket=1Say5lfE1%2FWnnYnvV%2Fo%2BEG25lxuQly5qT76ESRzqNbmW4IOQW2wESAlI%2BIMWln%2FB\n",
    "\n",
    "参考 XGBoost超详细推导，终于有人讲明白了！\n",
    "https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247486101&idx=1&sn=53d7d6b37f2c90a288d6b365af33761e&chksm=e870dbd8df0752ce7fe83cef50f7541bda1ae888e2256776654f36c81dd1524316cb62d81dc0&mpshare=1&scene=1&srcid=&sharer_sharetime=1574901170472&sharer_shareid=ce71cb8916e6563c3a833837c5e9bc18&key=a3e7a9e6351a3fdaf437d185dda38598353b25b449578050f7cbc3407ced7368cf73e65339d51ae561ff2d9001f0444021f0061fc17f79a6a8176aeef2bc48fd901a795e0074bcc756e97bc38999bc8f&ascene=1&uin=MTY0Mjc4NDkwMA%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&pass_ticket=1Say5lfE1%2FWnnYnvV%2Fo%2BEG25lxuQly5qT76ESRzqNbmW4IOQW2wESAlI%2BIMWln%2FB\n",
    "\n",
    "参考 数据竞赛利器XGBoost常见面试题集锦\n",
    "https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247485779&idx=1&sn=e77c36a34aadf9fef7d550765f5a7787&chksm=e870d81edf0751086268286be92992a87ab188b9e14e6d6a71244da76e21a500c7b1c8eb03be&mpshare=1&scene=1&srcid=&sharer_sharetime=1574901184592&sharer_shareid=ce71cb8916e6563c3a833837c5e9bc18&key=c6adf044e136fdb9b4c8dd0eb902b8219010f148e5d04e97b4efa4113e668f53ed2a48d20b43352e2c15402762512d031fc8ee65736a8b2a79283696cb7653b84892edb5125df5c1a980f27ad312fc09&ascene=1&uin=MTY0Mjc4NDkwMA%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&pass_ticket=1Say5lfE1%2FWnnYnvV%2Fo%2BEG25lxuQly5qT76ESRzqNbmW4IOQW2wESAlI%2BIMWln%2FB\n",
    "\n",
    "\n",
    "#### 1、介绍一下RandomForest、Adaboost、GBDT\n",
    "\n",
    "RandomForest：随机森林是 Bagging 的一个变体。其以决策树为基学习器，在决策树的训练过程中引入了随机属性选择。与 Bagging 相比，其基学习器的多样性不仅来自样本扰动，还来自属性扰动，最终集成的泛化性能进一步提升。\n",
    "\n",
    "Adaboost：Adaboost是一种 Boosting 方法，其核心思想有两点：a.提高前一轮训练中错误分类的的样本的权重，而降低哪些被正确分类样本的权重 b.加权多数表决的方法，加大分类误差率小的弱分类器的权重，减少分类误差率大的弱分类器的权重 . 最终得到一个多个弱学习器的加权组合。\n",
    "\n",
    "GBDT： Boosting Tree 是以分类树或回归树为基本分类器的提升方法，GBDT是 Boosting Tree 的 一种改进，利用损失函数的负梯度作为残差的近似值，解决了Boosting Tree 对一般损失函数优化困难的问题。\n",
    "\n",
    "#### 2、梯度提升决策树(GBDT)的原理是什么，为什么每次拟合的是一阶梯度，能推导一下吗\n",
    "\n",
    "GBDT是 Boosting Tree 的 一种改进，利用损失函数的负梯度作为残差的近似值。\n",
    "$$\n",
    "L(y,F(x)) = L(y,y^{t-1}+f_t(x))\\\\\n",
    "= L(y,y^{t-1}) + gf_t(x)+\\frac{1}{2}hf_t^2(x)   二阶泰勒展开\\\\\n",
    "g:一阶偏导 h：二阶偏导\\\\\n",
    "f_t(x)= -\\frac{g}{h} 取极值\\\\\n",
    "$$\n",
    "GDBT 算法假设 h = 1 ,$f_t(x) = -g$ ,通过拟合一阶梯度来近似取到误差极小值。\n",
    "\n",
    "#### 3、GBDT/xgboost每次获得的新一轮决策树模型，通常都要乘以系数，为什么\n",
    "\n",
    "系数是学习率，主要是为了削弱每棵树的影响，让后面有更大的学习空间。\n",
    "\n",
    "#### 4、xgboost和GBDT有什么区别\n",
    "\n",
    "a. GBDT 以 CART 作为基分类器，xgboost 还支持线性分类器.\n",
    "\n",
    "b. GBDT在优化时只用到一阶导数信息，xgboost 则对损失函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。（xgboost 支持自定义一阶和二阶导数，即自定义损失函数）.\n",
    "\n",
    "c. xgboost在代价函数里加入了正则项(对每棵树的叶子节点个数和权重都做了惩罚)，用于控制模型的复杂度。 \n",
    "\n",
    "d.Shrinkage（缩减），相当于学习速率（xgboost中的eta）。主要是为了削弱每棵树的影响，让后面有更大的学习空间.\n",
    "\n",
    "e.列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算.\n",
    "\n",
    "f.对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。\n",
    "\n",
    "g.可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。\n",
    "\n",
    " h.xgboost工具支持并行。\n",
    "\n",
    "#### 5、xgboost为了控制过拟合做了什么优化\n",
    "\n",
    "a.在损失函数里加入了正则项。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。\n",
    "\n",
    "b.增加 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。主要是为了削弱每棵树的影响，让后面有更大的学习空间.\n",
    "\n",
    "c.列抽样（column subsampling)\n",
    "\n",
    "#### 6、xgboost的并行化是如何做的\n",
    "\n",
    "xgboost的并行化的实现是通过决策树分裂过程中分裂结点特征的并行计算实现的，即并行每个特征在同层结点上的增益计算过程。\n",
    "\n",
    "* `xgboost在实现上支持并行化，这里的并行化并不是类似于rf那样树与树之间的并行化，xgboost同boosting方法一样，在树的粒度上是串行的，但是在构建树的过程中，也就是在分裂节点的时候支持并行化，比如同时计算多个属性的多个取值作为分裂特征及其值，然后选择收益最大的特征及其取值对节点分裂。`\n",
    "\n",
    "#### 7、xgboost的树生长时的精确分裂与近似分裂分别是怎么做的？\n",
    "\n",
    "精确分裂也叫作贪心分裂，是遍历所有特征中可能的分裂点位置。\n",
    "\n",
    "当数据量非常大难以被全部加载进内存时或者分布式环境下时，贪心算法将不再合适。近似分裂通过特征的分布，按照分位数确定一组候选分裂点，通过遍历所有的候选分裂点来找到最佳分裂点。分位数的确定考虑了样本点在当前特征下的权重，权重值取损失函数的二阶导数在该样本点下的值。\n",
    "\n",
    "* `分裂算法有两种，一种是精确的分裂，一种是近似分裂算法，精确分裂算法就是把每个属性的每个取值都当作一次阈值进行遍历，采用的决策树是CART。近似分裂算法是对每个属性的所有取值进行分桶，按照各个桶之间的值作为划分阈值，xgboost提出了一个特殊的分桶策略，一般的分桶策略是每个样本的权重都是相同 的，但是xgboost使每个样本的权重为损失函数在该样本点的二阶导`\n",
    "\n",
    "#### 8、xgboost对于缺失值，训练和预测的时候都是怎么处理的？\n",
    "\n",
    "xgboost将缺失值看作一类数据，通过计算缺失值分别归属到左右结点的增益来确定缺失值的默认划分方向。预测时直接使用默认方向。\n",
    "\n",
    "#### 9、xgboost是如何完成二分类的，多分类呢\n",
    "\n",
    "xgboost回归算法改造可实现分类\n",
    "\n",
    "二分类：损失函数采用logloss ，最终预测值为$y=\\frac{1}{1+e^{-F_m(x)}}$ 。\n",
    "\n",
    "多分类：损失函数是softmax 损失，m个分类类别会构建m个分类器\n",
    "\n",
    "#### 10、xgboost有什么缺点(level-wise和预排序等角度)\n",
    "\n",
    "a.level-wise 建树方式对当前层的所有叶子节点一视同仁，有些叶子节点分裂收益非常小，对结果没影响，但还是要分裂，加重了计算代价。\n",
    "\n",
    "b.预排序方法空间消耗比较大，不仅要保存特征值，也要保存特征的排序索引，同时时间消耗也大，在遍历每个分裂点时都要计算分裂增益(不过这个缺点可以被近似算法所克服)\n",
    "\n",
    "#### 11、xgboost 和 LightGBM 有什么不同\n",
    "\n",
    "不同主要有以下几点：\n",
    "\n",
    "a. LightGBM 采用 Histogram算法。\n",
    "\n",
    "b. LightGBM 采用带深度限制的Leaf-wise的叶子生长策略。\n",
    "\n",
    "c. LightGBM 直接支持类别特征。\n",
    "\n",
    "d. LightGBM 高效支持并行。目前支持特征并行和数据并行两种。\n",
    "\n",
    "#### 12、level-wise的生长和leaf-wise的生长有什么不同，优缺点是什么？\n",
    "\n",
    "Level-wise 过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。\n",
    "\n",
    "Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。\n",
    "\n",
    "#### 13、lightGBM有哪些优化点\n",
    "\n",
    "a. 更快的训练速度\n",
    "\n",
    "b. 更低的内存消耗\n",
    "\n",
    "c. 更好的准确率\n",
    "\n",
    "d.分布式支持，可以快速处理海量数据\n",
    "\n",
    "#### 14、能解释一下LightGBM里基于histogram的决策树算法吗?\n",
    "\n",
    "直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。\n",
    "\n",
    "#### 15、实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？\n",
    "\n",
    "- xgboost在每一层都动态构建直方图， 因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导),所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。\n",
    "- lightgbm有一些工程上的cache优化\n",
    "\n",
    "#### 16、LightGBM对类别型是怎么处理的?\n",
    "\n",
    "直接支持类别特征(Categorical Feature)，不需要进行0/1展开。在对类别特征计算分割增益的时候，不是按照数值特征那样由一个阈值进行切分，而是直接把其中一个类别当成一类，其他的类别当成另一类。这实际上与0/1展开的效果是一样的。\n",
    "\n",
    "`在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。`\n",
    "\n",
    "#### 17、lightgbm哪些方面做了并行?\n",
    "\n",
    "目前支持特征并行和数据并行的两种。特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。\n",
    "\n",
    "* feature parallel\n",
    "\n",
    "  `一般的feature parallel就是对数据做垂直分割（partiion data vertically，就是对属性分割），然后将分割后的数据分散到各个workder上，各个workers计算其拥有的数据的best splits point, 之后再汇总得到全局最优分割点。但是lightgbm说这种方法通讯开销比较大，lightgbm的做法是每个worker都拥有所有数据，再分割？（没懂，既然每个worker都有所有数据了，再汇总有什么意义？这个并行体现在哪里？？）`\n",
    "\n",
    "* data parallel\n",
    "\n",
    "  `传统的data parallel是将对数据集进行划分，也叫 平行分割(partion data horizontally)， 分散到各个workers上之后，workers对得到的数据做直方图，汇总各个workers的直方图得到全局的直方图。 lightgbm也claim这个操作的通讯开销较大，lightgbm的做法是使用”Reduce Scatter“机制，不汇总所有直方图，只汇总不同worker的不同feature的直方图(原理？)，在这个汇总的直方图上做split，最后同步。`\n",
    "\n",
    "#### 18、xgboost和LightGBM有哪些控制过拟合的手段，通常需要调整的参数有哪些?\n",
    "\n",
    "xgboost 参考题5.\n",
    "\n",
    "LightGBM\n",
    "\n",
    "- 设置较少的直方图数目，`max_bin`\n",
    "- 设置较小的叶节点数`num_leaves`\n",
    "- 设置参数`min_data_in_leaf`和`min_sum__hessian_in_leaf`\n",
    "- 使用bagging进行行采样`bagging_fraction`和`bagging_freq`\n",
    "- `feature_fraction`,列采样\n",
    "- 控制树深`max_depth`\n",
    "- 正则化\n",
    "  - `lambda_l1`\n",
    "  - `lambda_l2`\n",
    "  - 切分的最小收益`min_gain_to_split`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================================================================================================================================================\n",
    "=================\n",
    "## 1.5 KNN\n",
    "参考 深入浅出KNN算法（一） 介绍篇  \n",
    "https://zhuanlan.zhihu.com/p/61341071  \n",
    " KNN和K-mean有什么不同？  \n",
    " https://zhuanlan.zhihu.com/p/31580379"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================================================================================================================================================\n",
    "===============\n",
    "## 1.6 朴素贝叶斯\n",
    "参考\n",
    "\n",
    "\n",
    "### 朴素贝叶斯问题集\n",
    "\n",
    "#### 1.写一下贝叶斯公式?\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}\\\\\n",
    "P(A|B):A事件的后验概率\\\\\n",
    "P(A):A事件的先验概率\\\\\n",
    "P(B):B事件的先验概率\\\\\n",
    "P(B|A):B事件的后验概率\n",
    "$$\n",
    "#### 2.朴素贝叶斯是一个什么算法，能解释一下吗?\n",
    "\n",
    "朴素贝叶斯算法是基于贝叶斯公式的，通过计算各个类别的后验概率来判断分类，后验概率最大的类别是分类结果。贝叶斯公式应用于分类问题可用下式表示：\n",
    "$$\n",
    "P(c|x) =\\frac{P(x|c)*P(c)}{P(x)}\\\\ \n",
    "c:类别 \\\\\n",
    "x:特征\n",
    "$$\n",
    "由于$P(x|c)$是所有特征的联合概率，难以从训练样本直接估计，所以采用“条件独立性假设”，假设所有特征相互独立。则：\n",
    "$$\n",
    "P(c|x) =\\frac{P(x|c)*P(c)}{P(x)} = \\frac{P(c)}{P(x)}\\prod_{i=1}^d{P(x_i|c)}\\\\\n",
    "d:特征数目\n",
    "x_i:第i个特征上的取值\n",
    "$$\n",
    "\n",
    "由于对所有类别来说$P(x)$相同，则\n",
    "$$\n",
    "P(c|x) \\propto P(c)\\prod_{i=1}^d{P(x_i|c)}\n",
    "$$\n",
    "\n",
    "当样本数充足时，可容易估计出$P(c)$和$P(x_i|c)$，进而得到$P(c|x)$。\n",
    "\n",
    "#### 3.如果出现计数为0导致概率为0，这种情况在朴素贝叶斯计算里是怎么解决的?\n",
    "\n",
    "采用拉普拉斯平滑进行修正：\n",
    "$$\n",
    "\\hat{P}(c) = \\frac{|D_c|+1}{|D|+N}\\\\\n",
    "\\hat{P}(x_i|c) = \\frac{|D_{c,x_i}|+1}{|D|+N_i}\\\\\n",
    "N:训练集D中可能的类别数\\\\\n",
    "N_i:第i 个特征可能的取值数\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
