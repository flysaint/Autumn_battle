{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "### 3.1 常见题型\n",
    "### 3.2 RNN and LSTM\n",
    "### 3.3 FM and Deep FM\n",
    "### 3.4 Wide & Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================================\n",
    "=========================================\n",
    "### 深度学习习题集\n",
    "\n",
    "参考 如何通俗易懂地解释卷积？\n",
    "https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247486527&idx=1&sn=712ba2518092c53ba4d3db3dfdfd477d&chksm=e870dd72df075464acc72b6b8fee0823c60500754a5819f8c856ccbcb1e00a66d3ae7266f6a8&mpshare=1&scene=1&srcid=&sharer_sharetime=1574834218168&sharer_shareid=ce71cb8916e6563c3a833837c5e9bc18&key=c02f684d6b26de400c7614af4c541ff8fb77fc9415f75439622aee226143f30256c62c2cd75cfd16707c1e444e1745fa408c5b5a7e2c3c4a185e031eaeacfcbc25237d34a25446ab2d902e2d2e458195&ascene=1&uin=MTY0Mjc4NDkwMA%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&pass_ticket=1Say5lfE1%2FWnnYnvV%2Fo%2BEG25lxuQly5qT76ESRzqNbmW4IOQW2wESAlI%2BIMWln%2FB\n",
    "\n",
    "#### 1、介绍一下你了解的典型神经网络\n",
    "\n",
    "前馈神经网络：\n",
    "\n",
    "前馈神经网络采用一种单向多层结构。其中每一层包含若干个神经元，同一层的神经元之间没有互相连接，层间信息的传送只沿一个方向进行。其中第一层称为输入层。最后一层为输出层．中间为隐含层，简称隐层。隐层可以是一层。也可以是多层。\n",
    "\n",
    "卷积神经网络：\n",
    "\n",
    "卷积神经网络由一个或多个卷积层和顶端的全连接层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。\n",
    "\n",
    "循环神经网络：\n",
    "\n",
    "RNN是用来处理序列数据的神经网络，其引入了具有“记忆”性质的结构单元，计算除了本次的输入，还包括上一次的计算结果。\n",
    "\n",
    "#### 2、CNN是一种什么结构的网络\n",
    "\n",
    "CNN是常用于计算机视觉的一种前馈神经网络，包含卷积层这种利用局部感知野特性和权值共享保持学习能力降低参数的层次，和池化层这种具备采样操作的层次。常见层次还包括激励层和全连接层。\n",
    "\n",
    "#### 3、介绍一下CNN中的典型层次(layer)对应的操作和作用\n",
    "\n",
    "卷积计算层：卷积核在上一级输入层上通过逐一滑动窗口计算，卷积核中的每一个参数都相当于传统神经网络中的权值参数。通过卷积计算可以从原始数据中提取特征。\n",
    "\n",
    "激励层：利用sigmoid、Relu等非线性函数进行非线性变换。\n",
    "\n",
    "池化层：缩减数据维度，降低计算量，减少过拟合。常用的池化方式有 Max-Pooling 和 Mean-Pooling.\n",
    "\n",
    "#### 4、介绍一下VGG/inception/ResNet\n",
    "\n",
    "VGG的最主要的思想就是增加网络深度，减小卷积核尺寸（3*3）。\n",
    "\n",
    "inception的主要思想是如何在不增加计算成本的前提下扩展神经网络。引入了 Inception module，同时为了降低计算量，使用 1×1 卷积来执行降维。\n",
    "\n",
    "ResNet 的主要思想是通过学习残差，来构建深度更深的神经网络，达到更好的学习效果。\n",
    "\n",
    "#### 5、怎么判断神经网络是否过拟合，缓解神经网络过拟合的方法有哪些\n",
    "\n",
    "a.学习曲线判断是否过拟合。\n",
    "\n",
    "b.观察训练数据和验证数据的效果差异\n",
    "\n",
    "缓解过拟合的方法：\n",
    "\n",
    "- 正则化\n",
    "- Dropout\n",
    "- early stopping\n",
    "- 加数据\n",
    "- Batch Normalization也有一定的缓解过拟合作用\n",
    "\n",
    "#### 6、神经网络的优化算法有哪些(类似sgd、adam、adadelta...)\n",
    "\n",
    "sgd：随机梯度下降，对每个训练样本进行参数更新\n",
    "\n",
    "mini-batch sgd：利用每个批次中的n个训练样本进行更新\n",
    "\n",
    "adagrad: 学习率自动更新 $\\eta_w = \\frac{\\eta}{\\sqrt{\\sum_{i=0}^t{(g^i)^2}}}$\n",
    "\n",
    "adam:带有动量概念的学习率自动更新\n",
    "\n",
    "\n",
    "\n",
    "#### 7、神经网络为什么会出现梯度弥散(gradient vanish)问题，梯度爆炸呢\n",
    "\n",
    "梯度弥散：BP算法在梯度计算时如果层数过多，且激活函数是sigmoid函数，前面层的梯度值很小，意味着前面层失效，只有靠后的层有效，出现梯度弥散。\n",
    "\n",
    "梯度爆炸：当梯度值大于1时，连续相乘会使梯度累计值非常大，模型不稳定，出现梯度爆炸。\n",
    "\n",
    "#### 8、能介绍一下什么叫做Batch Normalization吗，它有什么样的作用\n",
    "\n",
    "Batch Normalization 是一个数据归一化操作，将数据归一化至均值为0，方差为1。同时为了避免破坏数据的特征分布，加入了变换重构参数，重构参数是可学习的。公式如下：\n",
    "\n",
    "![深度学习_batch_normalization_公式](/Users/duyongxiang/Documents/04_private/NLP学习计划/7月在线/机器学习习题集/image/深度学习_batch_normalization_公式.jpg)Batch \n",
    "\n",
    " Batch Normalization 可应用于一个神经网络的任何神经元上，论文中指出把BN置于激活函数前。\n",
    "\n",
    "注意：BN是作用于一个神经元的，处理的是同一个神经元 batch 输入。  \n",
    "\n",
    "\n",
    "\n",
    "BN本质上解决了反向传播过程中梯度消失的问题（改变数据分布），模型更快收敛。\n",
    "\n",
    "#### 9、ResNet层次那么深，为什么不那么容易出现梯度弥散\n",
    "\n",
    "ResNet 通过skip connection 使梯度直接回传，减少了深层网络的梯度弥散现象。通过高速通道，把原本的复合函数f(f(f(f(x))))变成加法操作，从而在BP求解时非连乘的形式，从数学上看不容易出现因为sigmoid而引起的梯度弥散。\n",
    "\n",
    "#### 10、有哪些你了解的激活函数\n",
    "\n",
    "sigmoid\n",
    "\n",
    "Relu\n",
    "\n",
    "tanh\n",
    "\n",
    "softmax\n",
    "\n",
    "maxout\n",
    "\n",
    "elu\n",
    "\n",
    "#### 11、为什么能用CNN做(短)文本分类\n",
    "\n",
    "CNN 的特点是可以提取局部特征，文本中词序列局部词的关系特征利用CNN可进行好的表示。(类似文本特征中n-gram的作用)\n",
    "\n",
    "#### 12、神经网络训练过程中，我们一般会调整哪些超参数\n",
    "\n",
    "学习率、 正则化参数、神经网络的层数、每层神经元个数、学习回合数 Epoch、minibatch、输出神经元的编码方式、损失函数、权重初始化方法、激活函数、训练数据规模。\n",
    "\n",
    "#### 13、训练过程中，batch size对loss变化的影响(batch size大和小，loss随时间有什么变化)\n",
    "\n",
    "一般来说，在合理的范围之内，越大的 batch size 使下降方向越准确，震荡越小；但 batch size 如果过大，则可能会出现局部最优的情况。loss不会再下降。\n",
    "\n",
    "小的 bath size 引入的随机性更大，如果过小会难以达到收敛，loss不会下降。(小batch size在loss曲线看，loss的抖动会非常大)\n",
    "\n",
    "#### 14、学习率对于训练过程中loss的影响\n",
    "\n",
    "如果学习速率太小，则会使收敛过慢，如果学习速率太大，则会导致损失函数振荡。\n",
    "\n",
    "#### 15、Dropout的为什么可以缓解过拟合\n",
    "\n",
    "a.Dropout 可以理解成构建多个神经网络取平均，不同的神经网络互相作用，可以减少各自的过拟合\n",
    "\n",
    "b.Dropout 使神经元间的连接并不固定，使特征可以被更多神经元学习到，增强了网络的鲁棒性。\n",
    "\n",
    "#### 16、深度学习中你见过哪些loss function，优缺点/分别在什么场景下使用\n",
    "\n",
    "均方差损失函数：梯度在误差过大或过小时会很小，网络学习会变慢。用于回归\n",
    "\n",
    "交叉熵损失函数：梯度为预测值和真实值的差距，网络学习速度均匀。用于分类。\n",
    "\n",
    "#### 17、深度学习中对不平衡样本的处理方式\n",
    "\n",
    "数据层面进行数据重采样(图像数据的data augmentation包括旋转、平移、裁剪等)\n",
    "\n",
    "算法层面可以使用代价敏感方法。包括代价敏感矩阵和代价敏感向量\n",
    "\n",
    "#### 18、RNN是一种什么样的神经网络\n",
    "\n",
    "RNN是用来处理序列数据的神经网络，其引入了具有“记忆”性质的结构单元，计算除了本次的输入，还包括上一次的计算结果。\n",
    "\n",
    "#### 19、介绍一下LSTM的结构(各种gate等…)，为什么LSTM能缓解BPTT中的gradient vanish\n",
    "\n",
    "LSTM 在传统的RNN结构单元的基础上做了改进，引入了遗忘门、输入门和输出门。（把公式列一下）\n",
    "\n",
    "LSTM 记忆状态由乘法操作改为加法操作，使记忆状态更稳定，梯度在记忆状态上的传递不容易消失。\n",
    "\n",
    "#### 20、GRU的结构和LSTM有什么区别\n",
    "\n",
    "a. GRU 将遗忘门和输入门合成了一个单一的更新门\n",
    "\n",
    "b. GRU 合并了 cell state 和 hidden state\n",
    "\n",
    "c.GRU 模型更简单，参数更少\n",
    "\n",
    "#### 21、深度学习用于文本分类，一般是如何去做的(以CNN和LSTM为例讲述)\n",
    "\n",
    "CNN: \n",
    "\n",
    "a.词表示为词向量，句子表示为词的组合，行成词向量组成的矩阵。\n",
    "\n",
    "b.卷积核大小为 filter_size*embedding_size，filter_size 代表一次卷积操作的单词个数。n卷积核得到n个列向量。\n",
    "\n",
    "c.卷积后进行池化，取列向量最大值，得到一个n维行向量。\n",
    "\n",
    "d.全连接层，转化为预测结果。\n",
    "\n",
    "LSTM:\n",
    "\n",
    "a.同样将文本表示为词向量矩阵\n",
    "\n",
    "b.数据接入LSTM或双向LSTM模型，最后一次的结果直接接全连接层softmax输出。\n",
    "\n",
    "\n",
    "\n",
    "#### 22、卷积神经网络中的卷积层，为什么需要padding\n",
    "\n",
    "a.保持边界信息，如果不加padding，边界信息只会被卷积核扫描一次\n",
    "\n",
    "b.可以通过padding对尺寸差异图片进行补齐\n",
    "\n",
    "**如果不加padding，每次经过卷积层，feature map都会变小，这样若干层卷积层之后，feature map就很小了，通过padding可以维持feature map的size**\n",
    "\n",
    "\n",
    "\n",
    "#### 23、全卷积的神经网络有什么优点，1*1的小卷积有什么作用\n",
    "\n",
    "全卷积的神经网络能够端到端得到每个像素的预测结果，同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。\n",
    "\n",
    "作用：\n",
    "\n",
    "a.实现跨通道的交互和信息整合\n",
    "\n",
    "b.降维或升维\n",
    "\n",
    "c.后接激励层可加入非线性\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "参考资料：\n",
    "\n",
    "深度学习第三期视频\n",
    "\n",
    "李宏毅《深度学习》\n",
    "\n",
    "《深度学习在文本分类中的作用》\n",
    "\n",
    "魏秀参《CNN book》\n",
    "\n",
    "[[What does 1x1 convolution mean in a neural network?](http://link.zhihu.com/?target=https%3A//stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network)]\n",
    "\n",
    "[[Inception modules: explained and implemented](http://link.zhihu.com/?target=https%3A//hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/)]\n",
    "\n",
    "[[How are 1x1 convolutions the same as a fully connected layer?](http://link.zhihu.com/?target=https%3A//datascience.stackexchange.com/questions/12830/how-are-1x1-convolutions-the-same-as-a-fully-connected-layer%3Fnewreg%3D6c118960b26c474fae9403d06ecd831b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 请详细说明 RNN and LSTM 的原理与注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
